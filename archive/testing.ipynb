{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'optimizers' from 'jax.experimental' (/Users/mariiashapo/anaconda3/envs/collocation_env/lib/python3.9/site-packages/jax/experimental/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grad, jit, random, value_and_grad\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m solve\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchebyshev_nodes_second_kind\u001b[39m(n, start, stop):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'optimizers' from 'jax.experimental' (/Users/mariiashapo/anaconda3/envs/collocation_env/lib/python3.9/site-packages/jax/experimental/__init__.py)"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, value_and_grad\n",
    "from jax.scipy.linalg import solve\n",
    "from jax.experimental import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def chebyshev_nodes_second_kind(n, start, stop):\n",
    "    k = jnp.arange(n)\n",
    "    x = jnp.cos(jnp.pi * k / (n - 1))\n",
    "    nodes = 0.5 * (stop - start) * x + 0.5 * (start + stop)\n",
    "    return jnp.sort(nodes)\n",
    "\n",
    "def lagrange_basis_node(xi):\n",
    "    n = len(xi)\n",
    "    L = jnp.eye(n)\n",
    "    return L\n",
    "\n",
    "def lagrange_basis(xi, x):\n",
    "    n = len(xi)\n",
    "    L = jnp.ones((n, len(x)))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                L = L.at[i, :].set(L[i, :] * (x - xi[j]) / (xi[i] - xi[j]))\n",
    "    return L\n",
    "\n",
    "def lagrange_basis_single(xi, x, j):\n",
    "    n = len(xi)\n",
    "    L = 1.0\n",
    "    for m in range(n):\n",
    "        if m != j:\n",
    "            L *= (x - xi[m]) / (xi[j] - xi[m])\n",
    "    return L\n",
    "\n",
    "def derivative_at_node(xi, j, h=1e-5):\n",
    "    x_j = xi[j]\n",
    "    forward = lagrange_basis_single(xi, x_j + h, j)\n",
    "    backward = lagrange_basis_single(xi, x_j - h, j)\n",
    "    derivative = (forward - backward) / (2 * h)\n",
    "    return derivative\n",
    "\n",
    "def lagrange_derivative(xi, weights):\n",
    "    n = len(xi)\n",
    "    D = jnp.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                D = D.at[i, j].set(weights[j] / weights[i] / (xi[i] - xi[j]))\n",
    "            else:\n",
    "                approx_derivative = derivative_at_node(xi, j)\n",
    "                D = D.at[i, j].set(approx_derivative)\n",
    "    return D\n",
    "\n",
    "def compute_weights(xi):\n",
    "    n = len(xi)\n",
    "    xi = jnp.array(xi)\n",
    "    weights = jnp.zeros(n)\n",
    "    for j in range(n):\n",
    "        terms = xi[j] - jnp.delete(xi, j)\n",
    "        product = jnp.prod(terms)\n",
    "        weights = weights.at[j].set(1.0 / product)\n",
    "    return weights\n",
    "\n",
    "def collocation_ode_solver(ode_system, initial_conditions, t, N, spacing=\"Chebyshev\", extra_params={}):\n",
    "    T_start, T_end = t[0], t[-1]\n",
    "    if spacing == \"Chebyshev\":\n",
    "        collocation_points = chebyshev_nodes_second_kind(N, T_start, T_end)\n",
    "    else:\n",
    "        collocation_points = jnp.linspace(T_start, T_end, N)\n",
    "        \n",
    "    phi = jnp.eye(N)\n",
    "    weights = compute_weights(collocation_points)\n",
    "    dphi_dt = lagrange_derivative(collocation_points, weights)\n",
    "\n",
    "    A = ode_system(dphi_dt, phi, extra_params)\n",
    "\n",
    "    b = jnp.zeros(2 * N)\n",
    "    b_aug = jnp.concatenate([b, jnp.array(initial_conditions)])\n",
    "\n",
    "    I_x1 = jnp.zeros((1, 2 * N))\n",
    "    I_x1 = I_x1.at[0, :N].set(phi[0, :])\n",
    "\n",
    "    I_x2 = jnp.zeros((1, 2 * N))\n",
    "    I_x2 = I_x2.at[0, N:].set(phi[0, :])\n",
    "\n",
    "    A_aug = jnp.vstack([A, I_x1, I_x2])\n",
    "\n",
    "    c = solve(A_aug.T @ A_aug, A_aug.T @ b_aug)\n",
    "\n",
    "    c1 = c[:N]\n",
    "    c2 = c[N:]\n",
    "    \n",
    "    lb = jnp.transpose(lagrange_basis(collocation_points, t))\n",
    "    x1 = lb @ c1\n",
    "    x2 = lb @ c2\n",
    "\n",
    "    solution = jnp.vstack([x1, x2]).T\n",
    "    return solution\n",
    "\n",
    "def init_mlp(layer_sizes, key):\n",
    "    keys = random.split(key, len(layer_sizes))\n",
    "    params = [(random.normal(k, (m, n)) * jnp.sqrt(2.0 / m), jnp.zeros(n)) for k, m, n in zip(keys, layer_sizes[:-1], layer_sizes[1:])]\n",
    "    return params\n",
    "\n",
    "def forward(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.tanh(jnp.dot(x, w) + b)\n",
    "    final_w, final_b = params[-1]\n",
    "    return jnp.dot(x, final_w) + final_b\n",
    "\n",
    "def neural_ode(params, t, state, extra_params):\n",
    "    return forward(params, state)\n",
    "\n",
    "@jit\n",
    "def loss_fn(params, t, states, true_states, extra_params):\n",
    "    preds = jnp.array([collocation_ode_solver(lambda dphi_dt, phi, extra_params: neural_ode(params, t, state, extra_params),\n",
    "                                               state, t, len(t), extra_params=extra_params) for state in states])\n",
    "    return jnp.mean((preds - true_states) ** 2)\n",
    "\n",
    "def main():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    layer_sizes = [2, 64, 64, 2]\n",
    "    params = init_mlp(layer_sizes, key)\n",
    "\n",
    "    example_ode_system = lambda dphi_dt, phi, params: jnp.block([\n",
    "        [dphi_dt, -phi],\n",
    "        [params['omega']**2 * phi, dphi_dt]\n",
    "    ])\n",
    "    \n",
    "    t_span = jnp.linspace(0, 10, 100)\n",
    "    N = 20\n",
    "    omega = 2.0\n",
    "    initial_conditions = (1.0, 0.0)\n",
    "    extra_params = {'omega': omega}\n",
    "    \n",
    "    true_solution = collocation_ode_solver(example_ode_system, initial_conditions, t_span, N, extra_params=extra_params)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(1e-3)\n",
    "    opt_state = opt_init(params)\n",
    "    \n",
    "    @jit\n",
    "    def step(i, opt_state):\n",
    "        params = get_params(opt_state)\n",
    "        value, grads = value_and_grad(loss_fn)(params, t_span, [initial_conditions], true_solution, extra_params)\n",
    "        opt_state = opt_update(i, grads, opt_state)\n",
    "        return opt_state, value\n",
    "\n",
    "    num_steps = 1000\n",
    "    for i in range(num_steps):\n",
    "        opt_state, value = step(i, opt_state)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Step {i}, Loss: {value}\")\n",
    "\n",
    "    trained_params = get_params(opt_state)\n",
    "\n",
    "    pred_solution = collocation_ode_solver(lambda dphi_dt, phi, params: neural_ode(trained_params, t_span, (dphi_dt, phi), params),\n",
    "                                           initial_conditions, t_span, N, extra_params=extra_params)\n",
    "    \n",
    "    plt.plot(t_span, true_solution[:, 0], label='True x1')\n",
    "    plt.plot(t_span, true_solution[:, 1], label='True x2')\n",
    "    plt.plot(t_span, pred_solution[:, 0], '--', label='Predicted x1')\n",
    "    plt.plot(t_span, pred_solution[:, 1], '--', label='Predicted x2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collocation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
